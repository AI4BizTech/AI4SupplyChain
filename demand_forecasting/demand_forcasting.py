# -*- coding: utf-8 -*-
"""Demand_Forcasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e1vQfdLfH8iznLSMlV_JV1Qhlp3e1HkC
"""

# The following code generates synthetic demand data when no real data is available
# Hybrid Approach: Rule-Based + GAN
# Step 1: Generate a basic dataset using rule-based simulation (as above).
# Step 2: Train a GAN on this synthetic dataset to add complexity and variability.
# Step 3: Use the GAN to generate refined synthetic data.


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# ==========================
# Step 1: Rule-Based Simulation
# ==========================
def generate_rule_based_data(days=365, base_demand=100, seasonal_peak=50, trend_growth=0.05, noise_range=10):
    """Generate synthetic demand data using rule-based simulation."""
    dates = pd.date_range(start="2025-01-01", periods=days, freq="D")
    trend = np.linspace(0, base_demand * trend_growth, days)
    seasonality = seasonal_peak * np.sin(2 * np.pi * (dates.dayofyear - 1) / 365)
    random_noise = np.random.uniform(-noise_range, noise_range, days)
    demand = base_demand + trend + seasonality + random_noise
    return pd.DataFrame({"Date": dates, "Demand": demand.round(2)})

# Generate rule-based data
rule_based_data = generate_rule_based_data()
rule_based_demand = rule_based_data["Demand"].values.astype(np.float32)

# Convert to PyTorch tensor
data_tensor = torch.from_numpy(rule_based_demand).view(-1, 1)
dataset = TensorDataset(data_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# Plot rule-based data
plt.figure(figsize=(10, 5))
plt.plot(rule_based_data["Date"], rule_based_data["Demand"], label="Rule-Based Demand")
plt.title("Rule-Based Synthetic Demand Data")
plt.xlabel("Date")
plt.ylabel("Demand")
plt.legend()
plt.grid()
plt.show()

# ==========================
# Step 2: Train GAN
# ==========================
class Generator(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 64),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(64),
            nn.Linear(64, 128),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(128),
            nn.Linear(128, 256),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(256),
            nn.Linear(256, 1)
        )

    def forward(self, z):
        return self.model(z)

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(1, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 64),
            nn.LeakyReLU(0.2),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.model(x)

# Initialize models and optimizers
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
latent_dim = 10
generator = Generator(latent_dim).to(device)
discriminator = Discriminator().to(device)

lr = 0.0002
optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))
criterion = nn.BCEWithLogitsLoss()

# Training loop
num_epochs = 10000
for epoch in range(num_epochs):
    for real_data, in dataloader:
        real_data = real_data.to(device)
        batch_size = real_data.size(0)

        # Train Discriminator
        optimizer_D.zero_grad()

        # Real data
        real_labels = torch.ones(batch_size, 1).to(device)
        real_output = discriminator(real_data)
        d_loss_real = criterion(real_output, real_labels)

        # Fake data
        z = torch.randn(batch_size, latent_dim).to(device)
        fake_data = generator(z)
        fake_labels = torch.zeros(batch_size, 1).to(device)
        fake_output = discriminator(fake_data.detach())
        d_loss_fake = criterion(fake_output, fake_labels)

        d_loss = (d_loss_real + d_loss_fake) / 2
        d_loss.backward()
        optimizer_D.step()

        # Train Generator
        optimizer_G.zero_grad()
        validity = discriminator(fake_data)
        g_loss = criterion(validity, torch.ones(batch_size, 1).to(device))
        g_loss.backward()
        optimizer_G.step()

    if epoch % 1000 == 0:
        print(f"Epoch {epoch} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}")

# ==========================
# Step 3: Generate Refined Synthetic Data
# ==========================
generator.eval()
with torch.no_grad():
    z = torch.randn(len(rule_based_demand), latent_dim).to(device)
    synthetic_demand = generator(z).cpu().numpy().flatten()

# Create DataFrame for synthetic data
synthetic_data = pd.DataFrame({
    "Date": rule_based_data["Date"],
    "Demand": synthetic_demand.round(2)
})

# Plot results
plt.figure(figsize=(10, 5))
plt.plot(rule_based_data["Date"], rule_based_data["Demand"], label="Original Rule-Based")
plt.plot(synthetic_data["Date"], synthetic_data["Demand"], label="GAN-Refined", alpha=0.7)
plt.title("Comparison of Demand Data")
plt.xlabel("Date")
plt.ylabel("Demand")
plt.legend()
plt.grid()
plt.show()